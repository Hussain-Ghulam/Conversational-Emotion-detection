{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,LSTM,Dropout,Dense,Bidirectional,Conv2D,Embedding,Masking,TimeDistributed,MaxPool2D\n",
    "from tensorflow.keras.layers import Reshape,Lambda,Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler,ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Load utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jisYe-h26-qq"
   },
   "outputs": [],
   "source": [
    "[_,_,_,_,_,_,embedding_matrix,tokenizer,label_index] = joblib.load('utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJZrn1EV6-rJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Label: {1: 'neutral', 2: 'joy', 3: 'surprise', 4: 'anger', 5: 'sadness', 6: 'fear', 7: 'disgust'}\n",
      "No of classes: 7\n",
      "Vocab size: 5243\n"
     ]
    }
   ],
   "source": [
    "index_to_label = {j:i for i,j in label_index.items()}\n",
    "print('Index Label:',index_to_label)\n",
    "classes = len(label_index)\n",
    "print('No of classes:',classes)\n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "print('Vocab size:',vocab_size)\n",
    "#Maximum words in any utterance\n",
    "max_sentence_length = 35\n",
    "#Maximum utterances in a dialogue\n",
    "max_utterances = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cp50E7GY6-rP"
   },
   "source": [
    "<h1>2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DY3AS9Nz6-rQ"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    inputs = Input(shape=(max_utterances,max_sentence_length),name='Input_layer',dtype='int32')\n",
    "\n",
    "    #Extract one utterance\n",
    "    def slicer(x, index):\n",
    "        return x[:,K.constant(index, dtype='int32'),:]\n",
    "    def slicer_output_shape(input_shape):\n",
    "        #shape == (Batch size, utterances, sentence_length)\n",
    "        shape = list(input_shape)\n",
    "        new_shape = (shape[0], shape[2])\n",
    "        return new_shape\n",
    "\n",
    "    #Add one dimension to embedding layer output\n",
    "    def reshaper(x):\n",
    "            return K.expand_dims(x, axis=3)\n",
    "\n",
    "    #Flatten\n",
    "    def flattener(x):\n",
    "        x = K.reshape(x, [-1, x.shape[1]*x.shape[3]])\n",
    "        return x\n",
    "    def flattener_output_shape(input_shape):\n",
    "        shape = list(input_shape)\n",
    "        #Multipy by 3 due to 3 filters\n",
    "        new_shape = (shape[0], 3*shape[3])\n",
    "        return new_shape\n",
    "\n",
    "    embedding = Embedding(input_dim=vocab_size,output_dim=300,input_length=max_sentence_length,\n",
    "                          name='Embedding_layer',trainable=False,weights=[embedding_matrix])\n",
    "    \n",
    "    conv_0 = Conv2D(512, kernel_size=(3,300), padding='valid', kernel_initializer='normal', activation='relu',name='Conv2D_0')\n",
    "    conv_1 = Conv2D(512, kernel_size=(4,300), padding='valid', kernel_initializer='normal', activation='relu',name='Conv2D_1')\n",
    "    conv_2 = Conv2D(512, kernel_size=(5,300), padding='valid', kernel_initializer='normal', activation='relu',name='Conv2D_2')\n",
    "    maxpool_0 = MaxPool2D(pool_size=(max_sentence_length - 3 + 1, 1), strides=(1,1), padding='valid',name='MaxPool2D_0')\n",
    "    maxpool_1 = MaxPool2D(pool_size=(max_sentence_length - 4 + 1, 1), strides=(1,1), padding='valid',name='MaxPool2D_1')\n",
    "    maxpool_2 = MaxPool2D(pool_size=(max_sentence_length - 5 + 1, 1), strides=(1,1), padding='valid',name='MaxPool2D_2')\n",
    "    \n",
    "    dense_func = Dense(512,activation='tanh',name='Dense_1')\n",
    "    drop = Dropout(0.5,name='Dropout_1')\n",
    "\n",
    "    cnn_output = []\n",
    "    for i in range(max_utterances):\n",
    "        # Extract utterance of dialogue\n",
    "        utter = Lambda(slicer, output_shape=slicer_output_shape, arguments={\"index\":i},\n",
    "                       name='Utterance_{}'.format(i))(inputs)\n",
    "        # Embedding layer\n",
    "        embed = embedding(utter)\n",
    "        # Expand dimensions \n",
    "        reshape = Lambda(reshaper,name='Reshape_Utterance_{}'.format(i))(embed)\n",
    "        # Concatenate outputs from three filters\n",
    "        concatenate = Concatenate(axis=1,name='Concat_Utterance_{}'.format(i))([maxpool_0(conv_0(reshape)),\n",
    "                                                                                maxpool_1(conv_1(reshape)),\n",
    "                                                                                maxpool_2(conv_2(reshape))])\n",
    "        # Flatten\n",
    "        flatten = Lambda(flattener, output_shape=flattener_output_shape,name='Flatten_Utterance_{}'.format(i))(concatenate)\n",
    "        # 512 dimensional vector for each utterances\n",
    "        dense_output = dense_func(flatten)\n",
    "        dropout = drop(dense_output)\n",
    "        cnn_output.append(dropout)\n",
    "\n",
    "    def stack(x):\n",
    "        return K.stack(x, axis=1)\n",
    "    # Stack cnn_output along axis 1 (timestep/utterance axis)\n",
    "    cnn_outputs = Lambda(stack,name='CNN_outputs')(cnn_output)\n",
    "    # Mask utterances which are zero padded\n",
    "    masked = Masking(mask_value=0,name='Masking')(cnn_outputs)\n",
    "    # 1 layer of Bidirectional LSTMs\n",
    "    lstm = LSTM(256, return_sequences = True, dropout=0.5, name='LSTM_1')(masked)\n",
    "    lstm = Dropout(0.5,name='Dropout_2')(lstm)\n",
    "    # Output layer for each utterances\n",
    "    # Class label from numbered from 1, hence 1 is added\n",
    "    output = Dense(classes+1,name='Output_layer')(lstm)\n",
    "    \n",
    "    model = Model(inputs, output,name='bc-LSTM')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vCXjULLx6-rV",
    "outputId": "22784db5-2979-4a7c-b17f-c23b21dc2220"
   },
   "outputs": [],
   "source": [
    "#Build model\n",
    "model = get_model()\n",
    "#Load weights\n",
    "model.load_weights('best_model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(dialogue):\n",
    "    \n",
    "    dialogue_length = len(dialogue)\n",
    "    \n",
    "    #Preprocess Utterances\n",
    "    #Decontraction of text\n",
    "    def decontracted(phrase):\n",
    "        \"\"\"\n",
    "        Returns decontracted phrases\n",
    "        \"\"\"\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    def preprocess(sentence):\n",
    "    \n",
    "        # process sentence\n",
    "        sentence = re.sub('\\x92','\\'',sentence)\n",
    "        # decontract sentence\n",
    "        sentence = decontracted(sentence)\n",
    "        # creating a space between a word and the punctuation following it\n",
    "        sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "        # remove extra spaces\n",
    "        sentence = sentence.strip()\n",
    "        # make lower case\n",
    "        sentence = sentence.lower()\n",
    "        return sentence\n",
    "    dia = np.array([preprocess(i) for i in dialogue])\n",
    "    \n",
    "    #Tokenize Utterances\n",
    "    def tokenizer_sentence(sentence,pad):\n",
    "        encoded_docs = tokenizer.texts_to_sequences([sentence])\n",
    "        padded_docs = tf.keras.preprocessing.sequence.pad_sequences(encoded_docs,maxlen=pad,padding='post')\n",
    "        return padded_docs\n",
    "    def tokenize_dialogue(dialogue,max_sentence_length,max_utterances):\n",
    "        utt = []\n",
    "        for i in dialogue:\n",
    "            utt.append(tokenizer_sentence(i,pad=max_sentence_length))\n",
    "        p = np.array(utt).reshape(-1,max_sentence_length)\n",
    "        # Pad utterances\n",
    "        q = np.zeros((max_utterances-len(utt),max_sentence_length),dtype='int8')\n",
    "        dia_array = np.concatenate([p,q])\n",
    "        return dia_array\n",
    "    dialogue_token = tokenize_dialogue(dia,max_sentence_length,max_utterances)\n",
    "    \n",
    "    #Get predictions from model\n",
    "    def predict_label(x):\n",
    "        a = model.predict(np.expand_dims(x,0))\n",
    "        return [i.argmax() for i in a[0]]\n",
    "    labels = predict_label(dialogue_token)[:dialogue_length]\n",
    "    \n",
    "    return dialogue,[index_to_label[i] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Why do all youre coffee mugs have numbers on ...</td>\n",
       "      <td>Mark</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>00:14:38,127</td>\n",
       "      <td>00:14:40,378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Oh. Thats so Monica can keep track. That way ...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>anger</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>00:14:40,629</td>\n",
       "      <td>00:14:47,385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Y'know what?</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>00:14:56,353</td>\n",
       "      <td>00:14:57,520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>Come on, Lydia, you can do it.</td>\n",
       "      <td>Joey</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0:10:44,769</td>\n",
       "      <td>0:10:46,146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Push!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0:10:46,146</td>\n",
       "      <td>0:10:46,833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance Speaker  \\\n",
       "0       1  Why do all youre coffee mugs have numbers on ...    Mark   \n",
       "1       2  Oh. Thats so Monica can keep track. That way ...  Rachel   \n",
       "2       3                                       Y'know what?  Rachel   \n",
       "3      19                     Come on, Lydia, you can do it.    Joey   \n",
       "4      20                                              Push!    Joey   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0  surprise  positive            0             0       3       19   \n",
       "1     anger  negative            0             1       3       19   \n",
       "2   neutral   neutral            0             2       3       19   \n",
       "3   neutral   neutral            1             0       1       23   \n",
       "4       joy  positive            1             1       1       23   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:14:38,127  00:14:40,378  \n",
       "1  00:14:40,629  00:14:47,385  \n",
       "2  00:14:56,353  00:14:57,520  \n",
       "3   0:10:44,769   0:10:46,146  \n",
       "4   0:10:46,146   0:10:46,833  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('MELD/test_sent_emo.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Predictions from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What? Honey.',\n",
       "       'Oh, I am, my side still hurts from when you crashed into me yesterday.',\n",
       "       'Oh God, I\\x92m so sorry.', 'I know.', 'Ow!!', 'Oh God!',\n",
       "       'Hey, you guys! Guess what?', 'Got a job on a river boat?',\n",
       "       \"Y'know what I didn\\x92t wear this suit for a year because you hated it. Well, guess what? You\\x92re not my girlfriend anymore so...\",\n",
       "       'Oh I see, so this suit is making a point.',\n",
       "       'Now that you\\x92re on you\\x92re own, you\\x92re free to look as stupid as you like.',\n",
       "       'You like it right?',\n",
       "       'Oh absolutely. I like it even more on you than I did on Colonel Sanders.  Ross! Ross! I\\x92m kidding!',\n",
       "       'Yeah, come here!', 'What-what was it you were gonna tell us?',\n",
       "       'Yeah. Oh! Was how you invented the cotton gin?!',\n",
       "       'Okay, good bye!'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = test[test['Dialogue_ID']==25]['Utterance'].values\n",
    "dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance: What? Honey.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Oh, I am, my side still hurts from when you crashed into me yesterday.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Oh God, Im so sorry.\n",
      "Emotion: sadness\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: I know.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Ow!!\n",
      "Emotion: surprise\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Oh God!\n",
      "Emotion: surprise\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Hey, you guys! Guess what?\n",
      "Emotion: surprise\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Got a job on a river boat?\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Y'know what I didnt wear this suit for a year because you hated it. Well, guess what? Youre not my girlfriend anymore so...\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Oh I see, so this suit is making a point.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Now that youre on youre own, youre free to look as stupid as you like.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: You like it right?\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Oh absolutely. I like it even more on you than I did on Colonel Sanders.  Ross! Ross! Im kidding!\n",
      "Emotion: anger\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Yeah, come here!\n",
      "Emotion: joy\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: What-what was it you were gonna tell us?\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Yeah. Oh! Was how you invented the cotton gin?!\n",
      "Emotion: surprise\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Okay, good bye!\n",
      "Emotion: joy\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Get predictions for dialogue\n",
    "dialogue, labels = get_emotion(dialogue)\n",
    "for i,j in zip(dialogue,labels):\n",
    "    print('Utterance:',i)\n",
    "    print('Emotion:',j)\n",
    "    print(120*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['But um, I don\\x92t think it\\x92s anything serious.',\n",
       "       'This sounds like a hernia. You have to\\x97you-you\\x97Go to the doctor!',\n",
       "       'No way!',\n",
       "       '\\x91Kay look, if I have to go to the doctor for anything it\\x92s gonna be for this thing sticking out of my stomach!',\n",
       "       'Why did I have to start working out again?', 'Damn you 15s!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = test[test['Dialogue_ID']==85]['Utterance'].values\n",
    "dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance: But um, I dont think its anything serious.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: This sounds like a hernia. You have toyou-youGo to the doctor!\n",
      "Emotion: anger\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: No way!\n",
      "Emotion: anger\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Kay look, if I have to go to the doctor for anything its gonna be for this thing sticking out of my stomach!\n",
      "Emotion: anger\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Why did I have to start working out again?\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Damn you 15s!\n",
      "Emotion: anger\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Get predictions for dialogue\n",
    "dialogue, labels = get_emotion(dialogue)\n",
    "for i,j in zip(dialogue,labels):\n",
    "    print('Utterance:',i)\n",
    "    print('Emotion:',j)\n",
    "    print(120*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"You got the clothes clean. Now that's the important part.\",\n",
       "       'Oh, I guess. Except everything looks like jammies now.',\n",
       "       \"Whoa, I'm sorry. Excuse me. We had this cart.\"], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue = test[test['Dialogue_ID']==59]['Utterance'].values\n",
    "dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance: You got the clothes clean. Now that's the important part.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Oh, I guess. Except everything looks like jammies now.\n",
      "Emotion: neutral\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Utterance: Whoa, I'm sorry. Excuse me. We had this cart.\n",
      "Emotion: sadness\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Get predictions for dialogue\n",
    "dialogue, labels = get_emotion(dialogue)\n",
    "for i,j in zip(dialogue,labels):\n",
    "    print('Utterance:',i)\n",
    "    print('Emotion:',j)\n",
    "    print(120*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
